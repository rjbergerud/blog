{
  
    
        "post0": {
            "title": "Review Notes for Folland's Advanced Calculus",
            "content": "Chapter 1 . Cauchy-Schwartz . I found the first piece of “magic”, magic meaning here an unintuitive explanation, in Folland’s proof of Cauchy’s Inequality . ∣v⋅w∣≤∣v∣∣w∣v,w∈Rn|v cdot w| leq |v||w| quad v,w in mathbb{R}^n∣v⋅w∣≤∣v∣∣w∣v,w∈Rn . The first step of the proof introduces the expression $ vert v - tw vert^2$ where $t in mathbb{R}$ seemingly out of nowhere. The truth is, there’s a buildup to this point that’s missing that would let us both come up with this form, and also see how to use it to prove the inequality. . There is a geometric picture that motivates this. The dot project of any two vectors is equivalently the projection either one of those vectors onto the other times the length of the other, so v⋅w=projwv∣w∣=projvw∣v∣v cdot w = proj_w v |w| = proj_v w |v|v⋅w=projw​v∣w∣=projv​w∣v∣ . where $proj_v$ is the function “project along $v$”. If you’ve ever thought it strange that ignoring the obvious commutative nature of the dot product, the last part of this equality seems unexpected, I’d suggest watching this visual explanation. . Now, thinking of the dot product in this way, clearly the length of the projection of $v$ onto any vector that doesn’t share the same direction will be less than the length of $v$. . . ∣v⋅w∣=∣projwv∣∣w∣≤∣v∣∣w∣|v cdot w| = |proj_w v| |w| leq |v||w|∣v⋅w∣=∣projw​v∣∣w∣≤∣v∣∣w∣ . We have a guiding explanation now, and we can use these to the notion of projections to guide the proof. If $v$ is to project along $w$, then $prov_w v = tw$ for some scalar $t$. What we’re going to make use of here is that the vector $n = v - vert proj_w v vert w$ always has non-negative length. . How can we define the function $proj_w$? Well, it’s the scalar multiple of $w$ which has the shortest distance from $v$. So we need to solve min⁡t∣v−wt∣ min_t |v - wt|mint​∣v−wt∣ . Where does this occur? Well, let’s square it (no worries, it will have the same minimum) which will make things more manageable . ∣v−wt∣2=(v−wt)⋅(v−wt)=∣w∣2t2−2(w⋅v)t+∣v∣2 begin{aligned} |v - wt|^2 = (v - wt) cdot(v - wt) &amp;= |w|^2t^2 - 2(w cdot v) t + |v|^2 end{aligned}∣v−wt∣2=(v−wt)⋅(v−wt)​=∣w∣2t2−2(w⋅v)t+∣v∣2​ . Finding the value $t$ for which the function is at a minimum requires finding values of $t$ for which the derivative of the quadratic function above is zero. . dfdt=2∣w∣2t0−2(w⋅v)=0  ⟹  t0=w⋅v∣w∣2 frac{df}{dt} = 2|w|^2t_0 - 2(w cdot v) = 0 implies t_0 = frac{w cdot v}{|w|^2}dtdf​=2∣w∣2t0​−2(w⋅v)=0⟹t0​=∣w∣2w⋅v​ . So now, we’ve come up with a working definition of a projection that doesn’t rely on all our euclidean tools like straight-edges and compasses. . projw(v)=v−w(w⋅v)∣w∣2proj_w (v) = v - w frac{(w cdot v)}{|w|^2}projw​(v)=v−w∣w∣2(w⋅v)​ . And we know that the length of this projection can never be negative, so . ∣projw(v)∣=∣v−wt0∣=∣w∣2(w⋅v)2∣w∣4−2(w⋅v)w⋅v∣w∣2+∣v∣2=−(w⋅v)2∣w∣2+∣v∣2 begin{aligned} |proj_w(v)| &amp;= |v - wt_0| &amp;= |w|^2 frac{(w cdot v)^2}{|w|^4} - 2(w cdot v) frac{w cdot v}{|w|^2} + |v|^2 &amp;= - frac{(w cdot v)^2}{|w|^2} + |v|^2 end{aligned}∣projw​(v)∣​=∣v−wt0​∣=∣w∣2∣w∣4(w⋅v)2​−2(w⋅v)∣w∣2w⋅v​+∣v∣2=−∣w∣2(w⋅v)2​+∣v∣2​ . A projection vector, like any vector, must always have positive length, therefore setting this as greater or equal to zero and a little manipulation shows the desired inequality squared . projw(v)≥0  ⟹  ∣w∣2∣v∣2≥(w⋅v)2proj_w(v) geq 0 implies |w|^2|v|^2 geq (w cdot v)^2projw​(v)≥0⟹∣w∣2∣v∣2≥(w⋅v)2 . Another way: . Another way to prove this inequality is to make sure the quadratic function in $t$ has no real roots. If it had real-roots, then $ vert v - tw vert$ would be negative valued for some $t$, which is impossible since it is a sum of squares. . t=−4(v⋅w)2±22(v⋅w)2−4∣v∣2∣w∣22∣w∣2t = frac{-4(v cdot w)^2 pm sqrt{2^2(v cdot w)^2 - 4|v|^2|w|^2}}{2|w|^2}t=2∣w∣2−4(v⋅w)2±22(v⋅w)2−4∣v∣2∣w∣2​​ well, we can see right away if we want complex solutions, the expression under the square root sign must stay negative. Wait a second - there’s the cauchy-schwartz inequality! . The cross product . Alright, if I remember correctly I’m pretty sure we’re going to see the cross product used in a few places in the books, so might as well get confortable with it early on. Instead of Folland’s BAM there you go style, I’d like to motivate the definition a little bit. .",
            "url": "https://rjbergerud.github.io/markdown/math/2021/04/06/folland_review_notes.html",
            "relUrl": "/markdown/math/2021/04/06/folland_review_notes.html",
            "date": " • Apr 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "What do I remember about Kalman filters?",
            "content": "What do I remember about Kalman Filters… . What are Kalman filters . Core idea: Kalman filters are used to combine different measurements of some state, make use of the the variance of each measurements errors distribution to make a “best” guess. An unbiased estimator? . Kalman filters are used to solve the problem: how to track a moving object. . More abstractly, Kalman filters are a way to track state when we have one or several measurements of the state, from possibly different sensors with different error distributions, as well as knowing the previous state. . Always working with gaussian errors allows us to to stay in a space. Gaussian distributions are a “closed” space under addition and scalar multiplication. If our way of combining distributions makes use of only these operations, then maybe we can always add information as it comes in. . Questions about Kalman Filters . Are there any restrictions on our motion model? . If there are restrictions on our motion model, what sort of problems have motion models that allow Kalman Filters to be applicable, and which don’t? . Is applying a Kalman filter in some sense the “optimal” way to make use of the information we have in order to infer the state of the object? . Are predictions by our motion model treated any differently from another sensor providing a measurement when we combine our information to make an estimate of state with the Kalman filter? How much weight do we give to our model vs our measurements? . Trying to reconstruct the gist of Kalman filters . Say we have two measurements $m_1, m_2$ with gaussian error terms, $ epsilon(0, sigma_1), epsilon(0, sigma_2)$, or alternatively written $m_1 sim N( mu_1, sigma_1)$, and if $m_2$ is measuring the same thing and it is unbiased, $m_1 sim N( mu_1, sigma_2)$. I can’t imagine we would be able to do anything if the errors where unbiased ($ mu neq 0$). How would we combine these two measurements in a way that gave an unbiased estimator? One might be to just take an average . m1+m22 frac{m_1 + m_2}{2}2m1​+m2​​ . and the estimator would be unbiased, since $E( frac{m_1 + m_2}{2}) = 0$. However, is this the really the best we can do? What if the variance of the second measure is huge, and the first measurement has nearly no variance? Our intuition would be to almost just take the first measurement and forget the second, or at least heavily weight towards the first. Why? Because the first has much lower variance. So can we combine the measurements in a way such that the variance of the combined variance is lower than the variance of either of the measurements on the their own? . Var(m1+m2)=∑((m1+m2)−(u1+u2))2p(m1,m2)=∑((m1−u1)+(m2−u1))2p(m1,m2) begin{aligned} Var(m_1 + m_2) &amp;= sum ((m_1 + m_2) - (u_1 + u_2))^2p(m_1, m_2) &amp;= sum((m_1 - u_1) + (m_2 - u_1))^2p(m_1, m_2) end{aligned}Var(m1​+m2​)​=∑((m1​+m2​)−(u1​+u2​))2p(m1​,m2​)=∑((m1​−u1​)+(m2​−u1​))2p(m1​,m2​)​ . Continuing to expand the lhs term in the product . =∑m1,m2∈D[(m1−u1)2+(m2−u2)2+(m1−u1)(m2−u2)]p(m1,m2)=σ12+σ22+2∑m1∑m2(m1−u1)(m2−u1)p(m1,m2) begin{aligned} &amp;= sum_{m_1,m_2 in D} big [(m_1 - u_1)^2 + (m_2 - u_2)^2 + (m_1 - u_1)(m_2 - u_2) big ]p(m_1, m_2) &amp;= sigma_1^2 + sigma_2^2 + 2 sum_{m_1} sum_{m_2} (m_1 - u_1)(m_2 - u_1)p(m_1, m_2) end{aligned}​=m1​,m2​∈D∑​[(m1​−u1​)2+(m2​−u2​)2+(m1​−u1​)(m2​−u2​)]p(m1​,m2​)=σ12​+σ22​+2m1​∑​m2​∑​(m1​−u1​)(m2​−u1​)p(m1​,m2​)​ . but that last term is just 2 times the covariance of $m_1$ and $m_2$! . tangent We can actually continue this step to expand the covariance to find a formula of covariance in terms of expected values. $$ begin{aligned} = sigma_1^2 + sigma_2^2 + 2u_1^2 + sum_{m_1} sum_{m_2} (m_1m_2 - (m_1 + m_2)u_1)p(m_1, m_2) &amp;= sigma_1^2 + sigma_2^2 + u_1^2 - u_1E(m_1) -u_1E(m_2) + E(m_1m_2) &amp;= sigma_1^2 + sigma_2^2 - u_1^2 + E(m_1m_2) end{aligned} $$ And to reduce $E(m_1m_2)$ further, $$E(u_1 + epsilon(0, sigma_1)(u_1 + epsilon(0, sigma_2))) = E(u_1^2 + epsilon_2u_1, + epsilon_1u_2 + epsilon_1 epsilon_2) = E(u_1^2) + E( epsilon_1 epsilon_2)) quad text{by linearity and $E( epsilon_i) = 0$}$$ If $ epsilon_1$ and $ epsilon_2$ are independent, then $E( epsilon_1 epsilon_2) = 0$, because we can write $p(m_1,m_2)$ as $p(m_1)p(m_2)$ and $$ sum_{ epsilon_1} sum_{ epsilon_2} epsilon_1 epsilon_2 p(m_1)p(m_2) = sum_{ epsilon_1} epsilon_1 p(m_1)E( epsilon_1) = 0 $$ Here’s the thing. If the covariance of these errors is zero, then our equation is much simplified. We’ll just have to remember later to keep this assumption in mind when we start to apply this to real-world problems. . But remember, we wanted to take a linear combination of these two measurements. Because $Var(am_1) = a^2Var(m_1)$, we have . Var(λm1+(1−λ)m2)=λ2σ12+(1−λ)2σ22Var( lambda m_1 + (1 - lambda) m_2) = lambda^2 sigma_1^2 + (1 - lambda)^2 sigma_2^2Var(λm1​+(1−λ)m2​)=λ2σ12​+(1−λ)2σ22​ . We parametrize our linear combination of measurements this way, because it is the only way to create an unbiased estimator if $ lambda$ is to be our coefficient of $m_1$. If we want to minimize the variance, we need to solve for the derivative with respect to $ lambda$ . 2λσ12−2(1−λ)σ22=0  ⟹  (1λ−1)=(σ1σ2)  ⟹  λ=1(σ1σ2)+12 lambda sigma_1^2 - 2(1 - lambda) sigma_2^2 = 0 implies left ( frac{1}{ lambda} - 1 right) = left ( frac{ sigma_1}{ sigma_2} right) implies lambda = frac{1}{ left( frac{ sigma_1}{ sigma_2} right) + 1}2λσ12​−2(1−λ)σ22​=0⟹(λ1​−1)=(σ2​σ1​​)⟹λ=(σ2​σ1​​)+11​ . A quick check with our intuition also tells us this seems right - if two measurements have the same variation in the error, our formula for $ lambda$ tells us they should be equally weighted. If the relative variance of $m_1$ is much lower, our $ lambda$ approaches 1, telling us to pretty much just use measurement 1 and ignore the second measurement. . However, notice in what we’ve done so far, we haven’t needed to assume our error is gaussian. All we’ve assumed is that our measurements are unbiased, and that the errors have zero covariance. . Now let’s apply this to an example. . Thoughts for example here: . temperature datasets - combining satellite and ground-station measured temperatures | . Adding in additional measurements as they come (or “realtime” filtering) . Let’s say we’re trying to measure some quantity that doesn’t vary over time, or vary very much other a short time period over which we’re taking consecutive samples. It could be something like sticking a thermometer in the ocean and reading out measurements every minute for an hour to try and get an accurate picture of ocean temperature in that place. Perhaps this isn’t such a great example, because the ocean isn’t uniform enough in temperature, with currents, solar radiation of the top layer, tides, etc. . Perhaps let’s contrive an example out of something we know is constant. Like the force of gravity at sea level at the lattitude of the equator. Now suppose every day we drop the same rubber ball 10 feet until some sensor detects it hits ground level. Our sensor is unbiased, but it’s error has variance $0.2s^2$. We want to keep performing this experiment until we have an estimate of the gravitational constant with variance $10^-3$. How many times do we repeat this experiment? . One way of doing this is to repeat the experiment, at each iteration finding some way to combine the variance of the current best estimate with the current trial. If at iteration $i$ our best estimate is $c_i$ with variance $ sigma_i$, we can treat this as if we just made a observation $c_i$ with a instrument that had error variance $ sigma_i$, and apply the above formula. . Kalman filtering for multiple variables . Now suppose instead of measuring just temperature, we’re measuring temperature and current flow at a stationary bouy in the ocean. . Notice that, while we could treat each of the variables separately, we’re missing out on some possibly important information. That is, it is likely that temperature and flow are correlated in this scenario. Perhaps the bouy is in an inlet, where water warms up, and hence when the tide flows out it is warmer than when it flows in. . To get a complete picture of variation in our system, we look at something called the covariance matrix. We generalize our notion of covariance from one dimension . E[(m1−μ1)2)]E[(m_1 - mu_1)^2)]E[(m1​−μ1​)2)] . to multiple dimensions . E[(m⃗−μ⃗)(m⃗−μ⃗)T] mathbf E[( vec{m} - vec{ mu})( vec{m} - vec{ mu})^T]E[(m . −μ . ​)(m . −μ . ​)T] . Or written in matrix form in 2 dimensions: . ∑m⃗[(m1−μ1)2(m1−μ1)(m2−μ2)(m1−μ1)(m2−μ2)(m2−μ2)2]p(m⃗) sum_{ vec{m}} begin{bmatrix} (m_1 - mu_1)^2 &amp; (m_1 - mu_1)(m_2 - mu_2) (m_1 - mu_1)(m_2 - mu_2) &amp; (m_2 - mu_2)^2 end{bmatrix}p( vec{m})m . ∑​[(m1​−μ1​)2(m1​−μ1​)(m2​−μ2​)​(m1​−μ1​)(m2​−μ2​)(m2​−μ2​)2​]p(m . ) . Now, we’ll repeat the investigation that we made early in this higher-dimensional space. What’s the covariance matrix of two measurements added together? . E[(m+n⃗−(μm+μn⃗))(m+n⃗−(μm+μn⃗))T]=∑m⃗[(m1+n1−(μm1+μn1))2(m1+n1−(μm1+μn1))(m2+n2−(μm2+μn2))(m1+n1−(μm1+μn1))(m2+n2−(μm2+μn2))(m2+n2−(μm2+μn2))2]p(m⃗) begin{aligned} mathbf E[( vec{m + n} - ( vec{ mu_m + mu_n}))( vec{m + n} - ( vec{ mu_m + mu_n}))^T] &amp;= sum_{ vec{m}} begin{bmatrix} (m_1 + n_1 - ( mu_{m_1} + mu_{n_1}))^2 &amp; (m_1 + n_1 - ( mu_{m_1} + mu_{n_1}))(m_2 + n_2 - ( mu_{m_2} + mu_{n_2})) (m_1 + n_1 - ( mu_{m_1} + mu_{n_1}))(m_2 + n_2 - ( mu_{m_2} + mu_{n_2})) &amp; (m_2 + n_2 - ( mu_{m_2} + mu_{n_2}))^2 end{bmatrix} p( vec{m}) end{aligned}E[(m+n . ​−(μm​+μn​ . ​))(m+n . ​−(μm​+μn​ . ​))T]​=m . ∑​[(m1​+n1​−(μm1​​+μn1​​))2(m1​+n1​−(μm1​​+μn1​​))(m2​+n2​−(μm2​​+μn2​​))​(m1​+n1​−(μm1​​+μn1​​))(m2​+n2​−(μm2​​+μn2​​))(m2​+n2​−(μm2​​+μn2​​))2​]p(m . )​ . Yikes. That’s looking like a handful to make sense of at first sight. However, if we look at just the diagonal entries, each one just repeats the signle-dimensional case we did earlier of calculating variance for two measurements added together. If we again make the assumption that measurements $m_i$ and $n_i$ are pairwise uncorrelated (have zero covariance), then each diagonal entry simplifies to $ sigma_{m_i}^2 + sigma_{n_i}^2$. The off-diagonal entries measure the cross-correlation between variables $m_i$ and $n_j$. Should these as well be zero? Our intuition says yes, given our example —-. . It’s possible that we have non-zero covariance on the non-diagonals though. Intuitively, remember our tidal example from above. Adding together the temperature/flow vectors from two different instruments will maintain a covariance between summed temperature and summed vector flow values. . However, we’re confusing two different notions here. One is that current flow and temperature are related, and hence should show a correlation. The other is that, at a given point in time where we assume that there is a fixed flow and temperature, two instruments (really, likely four), one providing measure $ vec{m}$ and the other providing measurement $ vec{n}$ have uncorrelated errors, both in components with the same indice, but also between components with different indices (e.g. flow $m_2$ measured from instrument 1 and temperature $n_1$ from instrument 2 should remain uncorrelated). . Instead of describing our estimator as a linear combination of measurements, we can also describe it as a linear operator (matrix) on our measurements . y⃗=Am⃗m⃗+An⃗n⃗ vec{y} = A_{ vec{m}} vec{m} + A_{ vec{n}} vec{n}y . ​=Am . ​m . +An . ​n . where the matrices are diagonal. . σm1+n1,m2+n22=∑m1,n1(m1+n1−(μm1+μn1))(m2+n2−(μm2+μn2))p(m,n)=∑m1,n1((m1−μm1)+(n1−μn1))((m2−μm2)+(n2−μn2)))p(m,n)=∑m1,n1[(m1−μm1)(m2−μm2)−(m1−μm1)(n1−μn1)−(m2−μm2)(n2−μn2)+(n1−μn1)(n2−μn2)]p(m1,n1)=σm1m22−σm1n22−σn1m22+σn1n22 begin{aligned} &amp; sigma^2_{m_1 + n_1, m_2 + n_2} =&amp; sum_{m_1,n_1}(m_1 + n_1 - ( mu_{m_1} + mu_{n_1}))(m_2 + n_2 - ( mu_{m_2} + mu_{n_2}))p(m,n) =&amp; sum_{m_1,n_1}((m_1 - mu_{m_1}) + (n_1 - mu_{n_1}))((m_2- mu_{m_2}) + (n_2 - mu_{n_2})))p(m,n) =&amp; sum_{m_1,n_1} big [(m_1 - mu_{m_1})(m_2 - mu_{m_2}) &amp; quad quad - (m_1 - mu_{m_1})(n_1 - mu_{n_1}) &amp; quad quad- (m_2 - mu_{m_2})(n_2 - mu_{n_2}) &amp; quad quad + (n_1 - mu_{n_1})(n_2 - mu_{n_2}) big ]p(m_1, n_1) =&amp; sigma^2_{m_1 m_2} - sigma^2_{m_1 n_2} - sigma^2_{n_1 m_2} + sigma^2_{n_1 n_2} end{aligned}====​σm1​+n1​,m2​+n2​2​m1​,n1​∑​(m1​+n1​−(μm1​​+μn1​​))(m2​+n2​−(μm2​​+μn2​​))p(m,n)m1​,n1​∑​((m1​−μm1​​)+(n1​−μn1​​))((m2​−μm2​​)+(n2​−μn2​​)))p(m,n)m1​,n1​∑​[(m1​−μm1​​)(m2​−μm2​​)−(m1​−μm1​​)(n1​−μn1​​)−(m2​−μm2​​)(n2​−μn2​​)+(n1​−μn1​​)(n2​−μn2​​)]p(m1​,n1​)σm1​m2​2​−σm1​n2​2​−σn1​m2​2​+σn1​n2​2​​ .",
            "url": "https://rjbergerud.github.io/blurb/2020/12/01/Kalman_intro.html",
            "relUrl": "/blurb/2020/12/01/Kalman_intro.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Rotation matrix",
            "content": "Rotation matrix . how to think about it intuitively? . A rotation matrix is just the result of two dot-products, the top row with our vector, and the bottom row with our vector. . But remember what dot-products are? They’re projections with scaling (link to 3Blue1Brown video here). . If you look at the top row dot product with our vector, think of the top row itself as a vector. What vector does it form? It forms the vector that would be the x-axis if the x-axis was one unit long, and had rotated $ theta$ degrees. Similar thing can be said about the bottom row and the y-axis. So a rotation matrix can just be constructed by rotating the x and y-axis, and finding the projection of our vector onto that, instead of thinking rotating our vectorand then finding the new projections. .",
            "url": "https://rjbergerud.github.io/blurb/2020/11/04/rotation_matrix.html",
            "relUrl": "/blurb/2020/11/04/rotation_matrix.html",
            "date": " • Nov 4, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Demonstration - lines in 3d project onto lines in the image plane",
            "content": "Projections . Just a few notes and a visual demonstrating that lines in the real-world project to lines on the image plane when using perspective projection. . . See the Pen Perspective projection by Row (@wfjoiwe) on CodePen.",
            "url": "https://rjbergerud.github.io/blurb/2020/10/11/image-projections.html",
            "relUrl": "/blurb/2020/10/11/image-projections.html",
            "date": " • Oct 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://rjbergerud.github.io/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Empirical Tests for Uniformness",
            "content": "Empirical Tests for Uniformness . These musings take off from chapter 7 of Simulation Modelling and Analysis. . The beginning of this chapter asks the question what makes a good arithmetic random number generator? In taking his question to the reader as real and not rhetorical, here is an idea I explored. . That it doesn’t “miss” numbers. Though this is unreasonable for arithmetic generators with finite period, the most ideal $Unif(0,1)$ generator should be able to generate given enough time, generate any given number in the interval $[0,1]$. Just the fact that generators spit out numbers at discrete-time intervals means that this is impossible though, given that it only makes countably many numbers on a uncountable set $[0,1]$. . So maybe some weaker version of this might be possible. Something that has the spirit of “that there are no gaps” that can exist in the sequence given enough time. That is, given any interval that were a subset of $[0,1]$, no matter how small, we could expect given enough time that our generator would yield a numbe r in that interval. Any of the generators like the linear congruential generators can’t do this because of their finite period. So we at least need a generator with infinite period. Such a generator can’t exist in an electronic computer because the computer can only assume finitely many states, and hence can only represent finitely-many states, and hence any generator must have a finite period. . So we’ve thought up an ideal properties of random number generators, but which can’t exist in practice. What then, after all, is Averill looking for in an ideal $Unif(0,1)$ generator? . No self-correlation (that is, $Cov(x_i, x_{i + j}) to 0$) | performance | can reproduce the random numbers (without storing them) | .",
            "url": "https://rjbergerud.github.io/markdown/inquiry-based-learning/2020/01/14/uniformness.html",
            "relUrl": "/markdown/inquiry-based-learning/2020/01/14/uniformness.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Motivated largely by a love of the natural world, I’m interested in working in climate solutions. I also enjoy revisiting bits of mathematics I learned in my undergrad, so that might reflect in some posts I make here. . I completed a BSc Honours in Mathematics from University of Victoria in 2014, and a MSc in Computer Science from Georgia Tech in 2021. . Note - This website is powered by fastpages 1. It seems to be a great tool for turning blog posts into interactive essays, an idea I first encountered here. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rjbergerud.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rjbergerud.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}